### SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition

### 背景

大部分的OCR识别方法都基于编解码框架；如下图，是目前主流的基于Attention的框架；其对于**图片模糊，字符残缺**等情况仍然难以识别。

![1596630853869](images\1596630853869.png)

因此论文提出**语义增强的编解码框架SEED**，来提高低质量场景文本识别的鲁棒性。

### 模型方法

模型架构包括：编码模块，语义模块，预训练语言模型，解码模块四个部分。模型创新的地方是将CNN+LSTM提取的特征F，拿出来送入一个语义模块，提取出语义信息S，再用语义信息S来初始化解码器。另为为提升效果，使用了预训练的word2Vec语言模型得到一个词向量，来监督预测出来的语义信息S。

![1596879263943](images\1596879263943.png)

论文在ASTER模型的基础上集成语义模块,得到的SE-ASTER模型如下：

1）将CNN+LSTM提取的特征h，展开成一维向量I，在经过两层线性层得到语义信息S

2）语义信息S经过一层线性层，用于初始化解码器的GRU单元。

3）模型损失包括预测的损失Lr和语义监督的损失Ls，语义损失由语义信息S和词向量的余弦相似度计得。

![1596879804961](images\1596879804961.png)

### 模型实验

主要做了三方面的实验：

* 语义模块的消融实验：

  结论：用全局信息初始化解码器+词向量监督两个步骤和起来效果最好。

![1596881957114](images\1596881957114.png)

* 识别字符残缺的鲁棒性实验

  结论：裁减图片后，有语义增强的模型精度下降最小，对残缺字符鲁棒性好

  ![1596882098817](images\1596882098817.png)

* SEED模型的泛化实验

  结论：在另一个先进模型SAR上集成语义模块，集成后精度有明显提升。（IC13效果差，作者的解释是，该数据集中低质量的图片少）

![1596882324539](images\1596882324539.png)