## OCR中文识别模型

### 模型情况

模型框架：CRNN + CTC模型；CRNN + Attend模型

数据集：310w数据（100w数字） + 160w稀少增强数据

### 模型优化

#### 有效果

* CTC框架和Attend框架
* **ResNet代替VGG提取图片特征**
* **按图片大小分层训练**

>坎1：验证精度96%，实际测评80%；模型在真实数据上泛化能力不足
>
>策略：校正过拟合（dropout，L2正则），扩大训练epoch，增加真实数据平衡分布

* 新增真实标注数据
* 收集训练难样本，（Focalloss和异常样本）
* 增强稀少字符平衡样本

>- 添加真实数据后效果有提升；
>  - 对短文本效果提高；语义对齐的错误大幅减少；**错误集中在个别字形上**；
>  - 问题：复杂的字识别错；特殊符号缺失导致错误；分号冒号混淆

#### 其他尝试

* 使用深的Resnet提高特征表示     X
* 参考DAN集成CAM模块，解耦对齐和解码过程  X
* **校正Attend机制的对齐误差FAN（需要字符标注）**
* 编码过程去掉LSTM全用CNN特征  ？
* **后处理改用beam search 代替贪心**    
* bert句向量集成     X

#### 待尝试

* 图片取反（二值化）+ 最大池化

* 对CNN卷积层-池化层进行加减   ？

* 构造2D特征图（小U型融合） + 2D Attend      [**特征表示**]

* 在模型特征输入和输出层加Dropout层（参考DAN代码） 

* 采用CTC+beam_search模型；后处理校正 ？

* 构建更好的backbone和多头注意力机制等先进的技术？

  

### 训练思路

目前，长短文本混合训练；大数据集学习率先用1训练3个epoch， 在0.1训练1个epoch， 0.01训练1个epoch；0.001训练1~2个epoch

* 交替长短文本训练，遍历不少于5个epoch

目前，以CRNN+Attend模型为主，扩增真实数据后第四版模型最佳

* **Guide CTC：**先训练一个好的CRNN+Attend模型，再固定encoder的权重，训练CTC模型



### 重要问题分析

* CTC和Attend模型的输出比较，如何训练和推理？
* pipeline改进：
  * 如何恢复低质量图片？如何对图片作高效的二值化处理？对边框是否纠正？
  * 如何优化预测搜索？ 怎么写规则校正字符？如何加纠错模型？
* 如何学习难样本（OHEM和GHM）
* 模型组件改进：
  * 优化CRNN特征提取？ 优化loss？优化字符对齐？使用模型级联？
  * 模型训练：多线程和多gpus训练

>算法问题：
>
>LSTM和GRU的原理比较，优劣和适用场合？
>
>CTC和Attend原理比较，优劣和适用场合？
>
>视觉特征模型的设计：
>
>​	模型主结构怎么设计，理由是什么？
>
>​    Avgpool和Maxpool的比较，小卷积和大卷积的比较，如何选用？
>
>​    ResNet残差层的理解？最新更高效的Net设计？
>
>​    CNN怎么做特征融合？
>
>  

### 快速测试的方案

tiny数据集20w， 验证集2k， 测评集3个

* baseline在tiny数据集上跑代码：

>lr=1，epoch=3,  batch=32
>
>重复跑2~3次，去其中6个模型，在真实测评集上测评；统计效果作为模型标准

* 在tiny数据上跑改进过的代码





### 纠错模型

#### 错字符定位问题

* 利用NN输出的字符置信度，但其很不准确（随机设定阈值有问题）

  * 收集另一个测试集的字符置信度分布；计算不同阈值下的P-R值
  * 定位问题希望查准率很高（不希望将错字判定为正确），但召回率也不能太低
  * 对于FP来说相当于没有纠错；改进量来自TN， 负作用来自FN（找回率太低，负作用太强）

* 假定已经定位到一个合适的错别字位置：取出字符串和对应位置（str，i）

  * 更具混淆字典查找易混淆的字，

    >统计模型对每个字的预测准确率（P-R和F1值），得到易识别字 | 难识别字
    >
    >得到每个字的置信度，划分预测正确的列表和错误的置信度列表， 计算合适的阈值分布






## 深度学习重要问题

数据分布完全可得的假设，在实践中往往是脆弱的。通常为模型添加新的“认知”，需要重新收集数据样本，然后如何能“累积”的让模型学到新的“认知”？而不用担心“学了这个忘了那个”的情况。如果新增小数据集也需要从大样本开始训练（无论是从头还是fine-tune）都是巨大的开销！

* 新学一个知识，要把旧知识的资料全看一遍！只是为了防止忘记，混淆旧知识？